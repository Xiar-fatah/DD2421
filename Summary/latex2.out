\BOOKMARK [1][-]{section.1}{Lecture 2}{}% 1
\BOOKMARK [2][-]{subsection.1.1}{Decision Tree}{section.1}% 2
\BOOKMARK [2][-]{subsection.1.2}{How to build a Tree}{section.1}% 3
\BOOKMARK [2][-]{subsection.1.3}{How to ask the best questions?}{section.1}% 4
\BOOKMARK [3][-]{subsubsection.1.3.1}{Gini impurity}{subsection.1.3}% 5
\BOOKMARK [3][-]{subsubsection.1.3.2}{Entropy}{subsection.1.3}% 6
\BOOKMARK [3][-]{subsubsection.1.3.3}{Information gain}{subsection.1.3}% 7
\BOOKMARK [3][-]{subsubsection.1.3.4}{Best question}{subsection.1.3}% 8
\BOOKMARK [2][-]{subsection.1.4}{Overfitting}{section.1}% 9
\BOOKMARK [3][-]{subsubsection.1.4.1}{Pruning}{subsection.1.4}% 10
\BOOKMARK [3][-]{subsubsection.1.4.2}{Validation set}{subsection.1.4}% 11
\BOOKMARK [1][-]{section.2}{Lecture 3}{}% 12
\BOOKMARK [2][-]{subsection.2.1}{Introduction}{section.2}% 13
\BOOKMARK [2][-]{subsection.2.2}{Curse of Dimensionality}{section.2}% 14
\BOOKMARK [2][-]{subsection.2.3}{The Bias-Variance Trade-off}{section.2}% 15
\BOOKMARK [1][-]{section.3}{Lecture 4}{}% 16
\BOOKMARK [2][-]{subsection.3.1}{Linear Regression, A Parametric Method}{section.3}% 17
\BOOKMARK [2][-]{subsection.3.2}{RANdom SAmpling Consensus}{section.3}% 18
\BOOKMARK [2][-]{subsection.3.3}{Disadvantages with RANSAC}{section.3}% 19
\BOOKMARK [2][-]{subsection.3.4}{k-NN Regression, A Non-parametric}{section.3}% 20
\BOOKMARK [2][-]{subsection.3.5}{Parametric or Non-parametric Methods?}{section.3}% 21
\BOOKMARK [2][-]{subsection.3.6}{Shrinkage Methods}{section.3}% 22
\BOOKMARK [2][-]{subsection.3.7}{Ridge Regression}{section.3}% 23
\BOOKMARK [2][-]{subsection.3.8}{The Lasso}{section.3}% 24
\BOOKMARK [1][-]{section.4}{Lecture 5}{}% 25
\BOOKMARK [2][-]{subsection.4.1}{Axiomatic definition of probabilities}{section.4}% 26
\BOOKMARK [2][-]{subsection.4.2}{Random \(Stochastic\) Variables}{section.4}% 27
\BOOKMARK [2][-]{subsection.4.3}{Types of Random Variables}{section.4}% 28
\BOOKMARK [2][-]{subsection.4.4}{Joint Probabilities}{section.4}% 29
\BOOKMARK [2][-]{subsection.4.5}{Marginalization}{section.4}% 30
\BOOKMARK [2][-]{subsection.4.6}{Conditional Probabilities}{section.4}% 31
\BOOKMARK [2][-]{subsection.4.7}{Common Distributions}{section.4}% 32
\BOOKMARK [2][-]{subsection.4.8}{Central Limit Theorem}{section.4}% 33
\BOOKMARK [2][-]{subsection.4.9}{Expectation}{section.4}% 34
\BOOKMARK [2][-]{subsection.4.10}{General Machine Learning Problem}{section.4}% 35
\BOOKMARK [2][-]{subsection.4.11}{Bayes's Rule}{section.4}% 36
\BOOKMARK [2][-]{subsection.4.12}{Probabilistic Regression}{section.4}% 37
\BOOKMARK [2][-]{subsection.4.13}{Selecting the most probable hypothesis}{section.4}% 38
\BOOKMARK [1][-]{section.5}{Lecture 6}{}% 39
\BOOKMARK [2][-]{subsection.5.1}{Introduction}{section.5}% 40
\BOOKMARK [2][-]{subsection.5.2}{Discriminative vs Generative Models}{section.5}% 41
\BOOKMARK [2][-]{subsection.5.3}{Parametric vs Non-parametric Inference}{section.5}% 42
\BOOKMARK [2][-]{subsection.5.4}{Maximum Likelihood \(ML\) Estimate}{section.5}% 43
\BOOKMARK [2][-]{subsection.5.5}{Curse of Dimensionality}{section.5}% 44
\BOOKMARK [2][-]{subsection.5.6}{Naive Bayes Classifier}{section.5}% 45
