\contentsline {section}{\numberline {1}Lecture 1}{7}{section.1}% 
\contentsline {section}{\numberline {2}Lecture 2}{7}{section.2}% 
\contentsline {subsection}{\numberline {2.1}Decision Tree}{7}{subsection.2.1}% 
\contentsline {subsection}{\numberline {2.2}How to build a Tree}{7}{subsection.2.2}% 
\contentsline {subsection}{\numberline {2.3}How to ask the best questions?}{7}{subsection.2.3}% 
\contentsline {subsubsection}{\numberline {2.3.1}Gini impurity}{7}{subsubsection.2.3.1}% 
\contentsline {subsubsection}{\numberline {2.3.2}Entropy}{8}{subsubsection.2.3.2}% 
\contentsline {subsubsection}{\numberline {2.3.3}Information gain}{8}{subsubsection.2.3.3}% 
\contentsline {subsubsection}{\numberline {2.3.4}Best question}{8}{subsubsection.2.3.4}% 
\contentsline {subsection}{\numberline {2.4}Overfitting}{8}{subsection.2.4}% 
\contentsline {subsubsection}{\numberline {2.4.1}Pruning}{8}{subsubsection.2.4.1}% 
\contentsline {subsubsection}{\numberline {2.4.2}Validation set}{8}{subsubsection.2.4.2}% 
\contentsline {section}{\numberline {3}Lecture 3}{9}{section.3}% 
\contentsline {subsection}{\numberline {3.1}Introduction}{9}{subsection.3.1}% 
\contentsline {subsection}{\numberline {3.2}Curse of Dimensionality}{9}{subsection.3.2}% 
\contentsline {subsection}{\numberline {3.3}The Bias-Variance Trade-off}{9}{subsection.3.3}% 
\contentsline {section}{\numberline {4}Lecture 4}{10}{section.4}% 
\contentsline {subsection}{\numberline {4.1}Linear Regression, A Parametric Method}{10}{subsection.4.1}% 
\contentsline {subsection}{\numberline {4.2}RANdom SAmpling Consensus}{11}{subsection.4.2}% 
\contentsline {subsection}{\numberline {4.3}Disadvantages with RANSAC}{11}{subsection.4.3}% 
\contentsline {subsection}{\numberline {4.4}k-NN Regression, A Non-parametric}{12}{subsection.4.4}% 
\contentsline {subsection}{\numberline {4.5}Parametric or Non-parametric Methods?}{12}{subsection.4.5}% 
\contentsline {subsection}{\numberline {4.6}Shrinkage Methods}{12}{subsection.4.6}% 
\contentsline {subsection}{\numberline {4.7}Ridge Regression}{12}{subsection.4.7}% 
\contentsline {subsection}{\numberline {4.8}The Lasso}{13}{subsection.4.8}% 
\contentsline {section}{\numberline {5}Lecture 5}{13}{section.5}% 
\contentsline {subsection}{\numberline {5.1}Axiomatic definition of probabilities}{13}{subsection.5.1}% 
\contentsline {subsection}{\numberline {5.2}Random (Stochastic) Variables}{13}{subsection.5.2}% 
\contentsline {subsection}{\numberline {5.3}Types of Random Variables}{14}{subsection.5.3}% 
\contentsline {subsection}{\numberline {5.4}Joint Probabilities}{14}{subsection.5.4}% 
\contentsline {subsection}{\numberline {5.5}Marginalization}{14}{subsection.5.5}% 
\contentsline {subsection}{\numberline {5.6}Conditional Probabilities}{15}{subsection.5.6}% 
\contentsline {subsection}{\numberline {5.7}Common Distributions}{15}{subsection.5.7}% 
\contentsline {subsection}{\numberline {5.8}Central Limit Theorem}{15}{subsection.5.8}% 
\contentsline {subsection}{\numberline {5.9}Expectation}{15}{subsection.5.9}% 
\contentsline {subsection}{\numberline {5.10}General Machine Learning Problem}{16}{subsection.5.10}% 
\contentsline {subsection}{\numberline {5.11}Bayes's Rule}{16}{subsection.5.11}% 
\contentsline {subsection}{\numberline {5.12}Probabilistic Regression}{16}{subsection.5.12}% 
\contentsline {subsection}{\numberline {5.13}Selecting the most probable hypothesis}{17}{subsection.5.13}% 
\contentsline {section}{\numberline {6}Lecture 6}{17}{section.6}% 
\contentsline {subsection}{\numberline {6.1}Introduction}{17}{subsection.6.1}% 
\contentsline {subsection}{\numberline {6.2}Discriminative modeling}{17}{subsection.6.2}% 
\contentsline {subsection}{\numberline {6.3}Generative Modeling}{17}{subsection.6.3}% 
\contentsline {subsection}{\numberline {6.4}Discriminative vs Generative Models}{17}{subsection.6.4}% 
\contentsline {subsection}{\numberline {6.5}Parametric vs Non-parametric Inference}{18}{subsection.6.5}% 
\contentsline {subsection}{\numberline {6.6}Maximum Likelihood (ML) Estimate}{18}{subsection.6.6}% 
\contentsline {subsection}{\numberline {6.7}Curse of Dimensionality}{18}{subsection.6.7}% 
\contentsline {subsection}{\numberline {6.8}Naive Bayes Classifier}{19}{subsection.6.8}% 
\contentsline {section}{\numberline {7}Lecture 7}{20}{section.7}% 
\contentsline {subsection}{\numberline {7.1}Maximum a Posteriori Estimation}{20}{subsection.7.1}% 
\contentsline {subsection}{\numberline {7.2}Limitation of Linear Regression}{20}{subsection.7.2}% 
\contentsline {subsection}{\numberline {7.3}Impact of different priors}{20}{subsection.7.3}% 
\contentsline {subsection}{\numberline {7.4}Bayesian estimation}{20}{subsection.7.4}% 
\contentsline {subsection}{\numberline {7.5}Bayesian Linear Regression}{21}{subsection.7.5}% 
\contentsline {subsection}{\numberline {7.6}Occam's Razor}{21}{subsection.7.6}% 
\contentsline {subsection}{\numberline {7.7}Limitations of Bayesian Non-parametric Methods}{21}{subsection.7.7}% 
\contentsline {subsection}{\numberline {7.8}Expectation Maximization}{22}{subsection.7.8}% 
\contentsline {subsection}{\numberline {7.9}EM properties}{22}{subsection.7.9}% 
\contentsline {subsection}{\numberline {7.10}Lecture 8}{22}{subsection.7.10}% 
\contentsline {section}{\numberline {8}Training a Linear Separator}{22}{section.8}% 
\contentsline {subsection}{\numberline {8.1}Percepton Learning}{22}{subsection.8.1}% 
\contentsline {subsection}{\numberline {8.2}Delta rule}{23}{subsection.8.2}% 
\contentsline {subsection}{\numberline {8.3}Linear Separation}{23}{subsection.8.3}% 
\contentsline {subsection}{\numberline {8.4}Structural Risk Minimization}{23}{subsection.8.4}% 
\contentsline {subsection}{\numberline {8.5}Support Vector Machine}{24}{subsection.8.5}% 
\contentsline {subsection}{\numberline {8.6}Kernels}{25}{subsection.8.6}% 
\contentsline {subsection}{\numberline {8.7}Slack}{25}{subsection.8.7}% 
\contentsline {section}{\numberline {9}Lecture 9}{25}{section.9}% 
\contentsline {section}{\numberline {10}Lecture 10}{26}{section.10}% 
\contentsline {subsection}{\numberline {10.1}Introduction}{26}{subsection.10.1}% 
\contentsline {subsection}{\numberline {10.2}The Wisdom of Crowds}{26}{subsection.10.2}% 
\contentsline {subsection}{\numberline {10.3}Combining Classifiers}{26}{subsection.10.3}% 
\contentsline {subsection}{\numberline {10.4}Ensemble Method: Baggning}{27}{subsection.10.4}% 
\contentsline {subsection}{\numberline {10.5}Ensemble Method: Forest}{27}{subsection.10.5}% 
\contentsline {subsection}{\numberline {10.6}Ensemble Method: Boosting}{27}{subsection.10.6}% 
\contentsline {subsection}{\numberline {10.7}Adaboost Algorithm }{28}{subsection.10.7}% 
\contentsline {section}{\numberline {11}Lecture 11}{28}{section.11}% 
\contentsline {subsection}{\numberline {11.1}Implementation of Principal Component Analysis}{28}{subsection.11.1}% 
\contentsline {subsection}{\numberline {11.2}Maximum variance criterion}{29}{subsection.11.2}% 
